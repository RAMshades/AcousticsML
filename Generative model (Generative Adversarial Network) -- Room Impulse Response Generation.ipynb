{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d232e172-9b2d-4f16-a2cb-d70f1cda8ba9",
   "metadata": {},
   "source": [
    "# Room Impulse Response Generation with GAN\n",
    "\n",
    "Reference:\n",
    "\n",
    "Ratnarajah, A., Tang, Z., & Manocha, D. (2021). IR-GAN: Room impulse response generator for far-field speech recognition. Proc. Interspeech.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "773d24b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchaudio\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6139c63d-baf5-47b4-9533-eca0e060d3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "## data\n",
    "\n",
    "# ! gdown --id 1YX1XEpJ2W1cZD4Dn7d5CRBVPOFLUKG4B --output ../data/RIR.zip\n",
    "# ! unzip -q ../data/RIR.zip -d ../data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdf723c6-de2b-4d93-82e1-b3872c1b819a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dataset for RIRs using torchaudio to load wav files\n",
    "class RIRDataset(Dataset):\n",
    "    def __init__(self, data_dir, slice_len=16384):\n",
    "        self.data_dir = data_dir\n",
    "        self.slice_len = slice_len\n",
    "        self.file_paths = [os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith('.wav')]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        wav_path = self.file_paths[idx]\n",
    "        waveform, sample_rate = torchaudio.load(wav_path)\n",
    "        # Ensure the waveform has the correct length, pad or truncate if necessary\n",
    "        if waveform.size(1) < self.slice_len:\n",
    "            waveform = torch.nn.functional.pad(waveform, (0, self.slice_len - waveform.size(1)))\n",
    "        elif waveform.size(1) > self.slice_len:\n",
    "            waveform = waveform[:, :self.slice_len]\n",
    "        return waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04298224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "930\n",
      "torch.Size([1, 16384])\n"
     ]
    }
   ],
   "source": [
    "dataset = RIRDataset('../data/RIR')\n",
    "\n",
    "print(len(dataset))\n",
    "print(dataset[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af5a1da-c724-46f5-a404-2b6c122aedd7",
   "metadata": {},
   "source": [
    "## Introduce the concept of generative model, especially GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ceceea-1303-4c7e-b191-43e57960a00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Conv1dTranspose(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=4, padding='same', upsample='zeros'):\n",
    "        super(Conv1dTranspose, self).__init__()\n",
    "        self.upsample = upsample\n",
    "        self.stride = stride\n",
    "        self.kernel_size = kernel_size\n",
    "        if self.upsample == 'zeros':\n",
    "            self.deconv = nn.ConvTranspose1d(in_channels, out_channels, kernel_size, stride=stride, padding=(kernel_size // 2))\n",
    "        elif self.upsample == 'nn':\n",
    "            self.conv = nn.Conv1d(in_channels, out_channels, kernel_size, stride=1, padding=(kernel_size // 2))\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.upsample == 'zeros':\n",
    "            return self.deconv(x)\n",
    "        elif self.upsample == 'nn':\n",
    "            x = F.interpolate(x, scale_factor=self.stride, mode='nearest')\n",
    "            return self.conv(x)\n",
    "\n",
    "\n",
    "class WaveGANGenerator(nn.Module):\n",
    "    def __init__(self, z_dim=100, slice_len=16384, nch=1, kernel_len=25, dim=64, use_batchnorm=False, upsample='zeros'):\n",
    "        super(WaveGANGenerator, self).__init__()\n",
    "        assert slice_len in [16384, 32768, 65536]\n",
    "        dim_mul = 16 if slice_len == 16384 else 32\n",
    "        self.dim_mul = dim_mul\n",
    "        self.use_batchnorm = use_batchnorm\n",
    "        self.upsample = upsample\n",
    "        \n",
    "        # Projection and reshape\n",
    "        self.fc = nn.Linear(z_dim, 16 * dim * dim_mul)\n",
    "        \n",
    "        # Layers\n",
    "        self.upconv_0 = Conv1dTranspose(dim * dim_mul, dim * (dim_mul // 2), kernel_len, stride=4, upsample=upsample)\n",
    "        self.upconv_1 = Conv1dTranspose(dim * (dim_mul // 2), dim * (dim_mul // 4), kernel_len, stride=4, upsample=upsample)\n",
    "        self.upconv_2 = Conv1dTranspose(dim * (dim_mul // 4), dim * (dim_mul // 8), kernel_len, stride=4, upsample=upsample)\n",
    "        self.upconv_3 = Conv1dTranspose(dim * (dim_mul // 8), dim * (dim_mul // 16), kernel_len, stride=4, upsample=upsample)\n",
    "        \n",
    "        if slice_len == 16384:\n",
    "            self.upconv_4 = Conv1dTranspose(dim * (dim_mul // 16), nch, kernel_len, stride=4, upsample=upsample)\n",
    "        elif slice_len == 32768:\n",
    "            self.upconv_4 = Conv1dTranspose(dim * (dim_mul // 16), dim, kernel_len, stride=4, upsample=upsample)\n",
    "            self.upconv_5 = Conv1dTranspose(dim, nch, kernel_len, stride=2, upsample=upsample)\n",
    "        elif slice_len == 65536:\n",
    "            self.upconv_4 = Conv1dTranspose(dim * (dim_mul // 16), dim, kernel_len, stride=4, upsample=upsample)\n",
    "            self.upconv_5 = Conv1dTranspose(dim, nch, kernel_len, stride=4, upsample=upsample)\n",
    "\n",
    "        self.batchnorm = nn.BatchNorm1d if use_batchnorm else lambda x: x\n",
    "        \n",
    "    def forward(self, z):\n",
    "        # FC and reshape for convolution\n",
    "        output = self.fc(z)\n",
    "        output = output.view(-1, self.dim_mul * 16, 16)\n",
    "        output = F.relu(self.batchnorm(output))\n",
    "\n",
    "        # Layer 0\n",
    "        output = F.relu(self.batchnorm(self.upconv_0(output)))\n",
    "        \n",
    "        # Layer 1\n",
    "        output = F.relu(self.batchnorm(self.upconv_1(output)))\n",
    "        \n",
    "        # Layer 2\n",
    "        output = F.relu(self.batchnorm(self.upconv_2(output)))\n",
    "        \n",
    "        # Layer 3\n",
    "        output = F.relu(self.batchnorm(self.upconv_3(output)))\n",
    "        \n",
    "        if hasattr(self, 'upconv_5'):\n",
    "            output = F.relu(self.batchnorm(self.upconv_4(output)))\n",
    "            output = torch.tanh(self.upconv_5(output))\n",
    "        else:\n",
    "            output = torch.tanh(self.upconv_4(output))\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class WaveGANDiscriminator(nn.Module):\n",
    "    def __init__(self, slice_len=16384, kernel_len=25, dim=64, use_batchnorm=False, phaseshuffle_rad=0):\n",
    "        super(WaveGANDiscriminator, self).__init__()\n",
    "        assert slice_len in [16384, 32768, 65536]\n",
    "        self.dim = dim\n",
    "        self.kernel_len = kernel_len\n",
    "        self.use_batchnorm = use_batchnorm\n",
    "        self.phaseshuffle_rad = phaseshuffle_rad\n",
    "\n",
    "        self.conv_0 = nn.Conv1d(1, dim, kernel_len, stride=4, padding=(kernel_len // 2))\n",
    "        self.conv_1 = nn.Conv1d(dim, dim * 2, kernel_len, stride=4, padding=(kernel_len // 2))\n",
    "        self.conv_2 = nn.Conv1d(dim * 2, dim * 4, kernel_len, stride=4, padding=(kernel_len // 2))\n",
    "        self.conv_3 = nn.Conv1d(dim * 4, dim * 8, kernel_len, stride=4, padding=(kernel_len // 2))\n",
    "        self.conv_4 = nn.Conv1d(dim * 8, dim * 16, kernel_len, stride=4, padding=(kernel_len // 2))\n",
    "\n",
    "        if slice_len in [32768, 65536]:\n",
    "            self.conv_5 = nn.Conv1d(dim * 16, dim * 32, kernel_len, stride=(4 if slice_len == 65536 else 2), padding=(kernel_len // 2))\n",
    "\n",
    "        self.fc = nn.Linear(dim * 16 * (slice_len // (4 ** 5)), 1)\n",
    "        self.batchnorm = nn.BatchNorm1d if use_batchnorm else lambda x: x\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = F.leaky_relu(self.conv_0(x), 0.2)\n",
    "        output = self.phaseshuffle(output)\n",
    "        output = F.leaky_relu(self.batchnorm(self.conv_1(output)), 0.2)\n",
    "        output = self.phaseshuffle(output)\n",
    "        output = F.leaky_relu(self.batchnorm(self.conv_2(output)), 0.2)\n",
    "        output = self.phaseshuffle(output)\n",
    "        output = F.leaky_relu(self.batchnorm(self.conv_3(output)), 0.2)\n",
    "        output = self.phaseshuffle(output)\n",
    "        output = F.leaky_relu(self.batchnorm(self.conv_4(output)), 0.2)\n",
    "\n",
    "        if hasattr(self, 'conv_5'):\n",
    "            output = F.leaky_relu(self.batchnorm(self.conv_5(output)), 0.2)\n",
    "\n",
    "        output = output.view(output.size(0), -1)\n",
    "        output = self.fc(output)\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def phaseshuffle(self, x):\n",
    "        if self.phaseshuffle_rad > 0:\n",
    "            phase = torch.randint(-self.phaseshuffle_rad, self.phaseshuffle_rad + 1, (1,)).item()\n",
    "            if phase > 0:\n",
    "                x = F.pad(x, (phase, 0), mode='reflect')\n",
    "                x = x[:, :, :-phase]\n",
    "            elif phase < 0:\n",
    "                x = F.pad(x, (0, -phase), mode='reflect')\n",
    "                x = x[:, :, -phase:]\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8452d0cf-a424-4db0-b701-6a5eb973e2b9",
   "metadata": {},
   "source": [
    "## An example use of sound field reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd543b72-719a-49cc-bfb4-b0688a449619",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Function for training the GAN\n",
    "def train_gan(data_dir, train_batch_size=64, epochs=20, latent_dim=100, lr=0.0002, data_slice_len=16384):\n",
    "    # Load dataset\n",
    "    dataset = RIRDataset(data_dir, slice_len=data_slice_len)\n",
    "    dataloader = DataLoader(dataset, batch_size=train_batch_size, shuffle=True)\n",
    "    \n",
    "    # Instantiate generator and discriminator\n",
    "    generator = WaveGANGenerator()\n",
    "    discriminator = WaveGANDiscriminator()\n",
    "    \n",
    "    # Define loss function and optimizers\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer_g = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    optimizer_d = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        for i, real_data in enumerate(dataloader):\n",
    "            batch_size = real_data.size(0)\n",
    "            \n",
    "            # Train discriminator\n",
    "            z = torch.randn(batch_size, latent_dim)\n",
    "            fake_data = generator(z)\n",
    "            \n",
    "            # Labels\n",
    "            real_labels = torch.ones(batch_size, 1)\n",
    "            fake_labels = torch.zeros(batch_size, 1)\n",
    "            \n",
    "            # Discriminator loss on real data\n",
    "            optimizer_d.zero_grad()\n",
    "            d_real = discriminator(real_data)\n",
    "            d_real_loss = criterion(d_real, real_labels)\n",
    "            \n",
    "            # Discriminator loss on fake data\n",
    "            d_fake = discriminator(fake_data.detach())\n",
    "            d_fake_loss = criterion(d_fake, fake_labels)\n",
    "            \n",
    "            # Combine losses\n",
    "            d_loss = d_real_loss + d_fake_loss\n",
    "            d_loss.backward()\n",
    "            optimizer_d.step()\n",
    "            \n",
    "            # Train generator\n",
    "            optimizer_g.zero_grad()\n",
    "            d_fake = discriminator(fake_data)\n",
    "            g_loss = criterion(d_fake, real_labels)  # Fool the discriminator\n",
    "            g_loss.backward()\n",
    "            optimizer_g.step()\n",
    "            \n",
    "            # Print progress\n",
    "            if i % 10 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{epochs}], Step [{i+1}/{len(dataloader)}], \\\n",
    "                      D Loss: {d_loss.item():.4f}, G Loss: {g_loss.item():.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7717d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set default values for training parameters\n",
    "data_dir = './data/rir_wavs'  # Directory containing RIR wav files\n",
    "train_batch_size = 64\n",
    "epochs = 20\n",
    "latent_dim = 100\n",
    "lr = 0.0002\n",
    "data_slice_len = 16384\n",
    "\n",
    "# Create training directory if it doesn't exist\n",
    "if not os.path.exists(\"./train_dir\"):\n",
    "    os.makedirs(\"./train_dir\")\n",
    "\n",
    "# Train GAN\n",
    "train_gan(data_dir, train_batch_size, epochs, latent_dim, lr, data_slice_len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c233907-5e26-45af-8242-7d1d6e7be5d6",
   "metadata": {},
   "source": [
    "## Extended Reading\n",
    "\n",
    "\n",
    "Ratnarajah, A., Zhang, S. X., Yu, M., Tang, Z., Manocha, D., & Yu, D. (2022, May). FAST-RIR: Fast neural diffuse room impulse response generator. In ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 571-575). IEEE.\n",
    "\n",
    "Fernandez-Grande, E., Karakonstantis, X., Caviedes-Nozal, D., & Gerstoft, P. (2023). [Generative models for sound field reconstruction.](https://pubs.aip.org/asa/jasa/article/153/2/1179/2866890) The Journal of the Acoustical Society of America, 153(2), 1179-1190."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a896d99-e179-4a42-baae-71b16445c9b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
